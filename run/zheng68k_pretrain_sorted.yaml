seed:
  seed_value: 1234
tokenizer:
  _target_: bmfm_targets.config.TokenizerConfig
  identifier: all_genes
fields:
- _target_: bmfm_targets.config.FieldInfo
  field_name: genes
- _target_: bmfm_targets.config.FieldInfo
  field_name: expressions
  is_masked: false
  tokenization_strategy: continuous_value_encoder
  encoder_kwargs:
    kind: mlp_with_special_token_embedding
    zero_as_special_token: true
  decode_modes:
    wced:
      vocab_field: genes
      logit_outputs:
      - mse
      - is_zero_bce
data_module:
  _target_: bmfm_targets.training.data_module.DataModule
  _partial_: true
  num_workers: 12
  mlm: true
  collation_strategy: language_modeling
  shuffle: true
  max_length: 1024
  sequence_order: sorted
  switch_ratio: 0.0
  batch_size: 40
  limit_genes: protein_coding
  log_normalize_transform: true
  transform_datasets: false
  pad_zero_expression_strategy: batch_wise
  masking_strategy:
    _target_: bmfm_targets.training.masking.strategy.WCEDMasker
    _partial_: true
model:
  _target_: bmfm_targets.config.SCBertConfig
  _partial_: true
  num_hidden_layers: 12
  num_attention_heads: 12
  intermediate_size: 3072
  hidden_act: gelu
  hidden_size: 768
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  initializer_range: 0.02
  layer_norm_eps: 1.0e-12
  pad_token_id: 0
  use_cache: true
  attention: torch
  checkpoint: null
trainer:
  _target_: bmfm_targets.config.TrainerConfig
  warmup_steps: 1000
  weight_decay: 0.01
  learning_rate: 0.0001
  betas:
  - 0.9
  - 0.99
  epsilon: 1.0e-08
  lr_decay_steps: null
  losses:
  - field_name: expressions
    name: mse
    ignore_zero: true
    wced_target: input_genes
  - field_name: expressions
    name: is_zero_bce
    wced_target: input_genes
  - field_name: expressions
    name: mse
    ignore_zero: true
    wced_target: non_input_genes
  - field_name: expressions
    name: is_zero_bce
    wced_target: non_input_genes
  batch_prediction_behavior: track
task:
  _target_: bmfm_targets.config.TrainingTaskConfig
  default_root_dir: /dccstor/bmfm_gene_diff/users/martinaina/results/experiments/zheng68k/pretrain/mlm/random
  max_epochs: 5
  precision: 16-mixed
  val_check_interval: 100
  gradient_clip_val: 0.5
  accelerator: gpu
  max_steps: -1
  tf32_mode: medium
  freeze_encoder: false
  resume_training_from_ckpt: false
  checkpoints_every_n_train_steps: 10000
  limit_val_batches: 256