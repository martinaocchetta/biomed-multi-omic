import atexit
import math
import os
import tempfile
from pathlib import Path

import torch
from tokenizers import Tokenizer
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
from transformers.models.bert import BertTokenizerFast
from transformers.tokenization_utils_base import (
    BatchEncoding,
    PaddingStrategy,
    TensorType,
    TruncationStrategy,
)
from transformers.utils import logging

from ..config.tokenization_config import FieldInfo
from . import MultiFieldVocabulary
from .multifield_instance import MultiFieldInstance

logger = logging.get_logger(__name__)


def _stringify(this_input, is_split_into_words=True):
    if isinstance(this_input, list):
        if len(this_input) > 0 and isinstance(this_input[0], str):
            return this_input
        return list(map(str, this_input))
    elif isinstance(this_input, str):
        if is_split_into_words:
            return [this_input]
        else:
            return this_input
    else:
        raise ValueError(f"Unsupported tokenization input type {type(this_input)}")


class MultiFieldTokenizer:
    """
    A tokenizer that can handle multiple fields of input.
    Class Attributes:
        tokenizers (dict[PreTrainedTokenizerFast|PreTrainedTokenizer]): The sub tokenizers.
        load_relative=True : load subtokenizers from a "tokenizers" sub directory next to the top tokenizer.
    """

    SUB_TOKENIZER_CLASS = BertTokenizerFast

    def __init__(
        self,
        name_or_path=None,
        field_to_tokenizer_map: dict[str, str | Path] = None,
        load_relative=True,
        default_field=None,
        **kwargs,
    ):
        if field_to_tokenizer_map:
            field_to_tokenizer_map = {
                key: str(value) for key, value in field_to_tokenizer_map.items()
            }
        elif not name_or_path:
            raise ValueError(
                "Multifield tokenizer init requires either a name_or_path or field_to_tokenizer_map"
            )
        else:
            load_relative = True

        self.base_dir = Path(name_or_path) / "tokenizers" if load_relative else None
        self.name_or_path = name_or_path
        self.field_to_tokenizer_map = field_to_tokenizer_map
        self.tokenizers = {}
        self.kwargs = kwargs
        self.default_field = default_field
        self.load_all_subtokenizers()

    def save_pretrained(
        self,
        save_directory: str | os.PathLike,
        save_subtokenizers: bool = True,
        filename_prefix: str = None,
        **kwargs,
    ):
        if filename_prefix is not None:
            Warning(
                "file prefix no longer supported for multiFieldTokenizer and will be ignored"
            )
        save_directory = Path(save_directory)
        if save_subtokenizers:
            (save_directory / "tokenizers").mkdir(exist_ok=True, parents=True)
            for field_name in self.tokenizers.keys():
                self.get_field_tokenizer(field=field_name).save_pretrained(
                    save_directory=save_directory / "tokenizers" / field_name
                )

    @classmethod
    def from_pretrained(cls, name_or_path, **kwargs):
        kwargs.update({"local_files_only": True})

        return MultiFieldTokenizer(name_or_path=name_or_path, **kwargs)

    @classmethod
    def from_old_multifield_tokenizer(
        cls, name_or_path, save_converted_tokenizer_back=False, **kwargs
    ):
        """
        convert an old multifield tokenizer into the new format.

        Args:
        ----
            name_or_path (str): path to tokenizer dir
            save_converted_tokenizer_back (bool, optional): If True, saves the converted tokenizer in the directory of the input tokenizer. Defaults to False.

        Returns:
        -------
            MultiFieldTokenizer: the converted tokenizer

        """
        if not (Path(name_or_path) / "multifield_vocab.json").exists():
            raise ValueError(f"Can not find a multifield tokenizer in {name_or_path}")
        vocab = MultiFieldVocabulary.load_from_json(
            str(Path(name_or_path) / "multifield_vocab.json")
        )
        fields = vocab.get_fields()
        field_to_tokenizer_map = {}
        with tempfile.TemporaryDirectory() as tmpdir:
            Path(tmpdir).mkdir(exist_ok=True)
            empty_vocab_file_path = Path(tmpdir) / "empty_vocab.txt"
            empty_vocab_file_path.touch(exist_ok=True)

            for field_name in fields:
                id_to_token = {
                    v[1]: k for k, v in vocab.token_to_ids[field_name].items()
                }
                fast_tokenizer = cls.SUB_TOKENIZER_CLASS(
                    vocab_file=empty_vocab_file_path
                )
                fast_tokenizer.add_tokens(
                    [id_to_token[i] for i in range(len(id_to_token))]
                )

                updated_vocab = fast_tokenizer.get_vocab()
                sorted_vocab = [
                    key
                    for key, idx in sorted(
                        updated_vocab.items(), key=lambda item: item[1]
                    )
                ]
                with open(f"{tmpdir}/created_vocab.txt", "w") as vocab_file:
                    vocab_file.write("\n".join(sorted_vocab) + "\n")

                updated_tokenizer = BertTokenizerFast(
                    vocab_file=f"{tmpdir}/created_vocab.txt"
                )
                field_to_tokenizer_map[field_name] = field_name

                updated_tokenizer.save_pretrained(
                    save_directory=Path(tmpdir) / "tokenizers" / field_name
                )
            full_tokenizer = cls.from_pretrained(name_or_path=tmpdir, **kwargs)
            if save_converted_tokenizer_back:
                full_tokenizer.save_pretrained(save_directory=name_or_path)
                full_tokenizer = cls.from_pretrained(name_or_path)
        return full_tokenizer

    @classmethod
    def convert_tokenizer_to_bert(
        cls, name_or_path, save_converted_tokenizer_back=False, **kwargs
    ):
        loaded_tokenizer = AutoTokenizer.from_pretrained(name_or_path)
        if isinstance(loaded_tokenizer, cls.SUB_TOKENIZER_CLASS):  # nothing to do
            return loaded_tokenizer

        with tempfile.TemporaryDirectory() as tmpdir:
            Path(tmpdir).mkdir(exist_ok=True)
            empty_vocab_file_path = Path(tmpdir) / "empty_vocab.txt"
            empty_vocab_file_path.touch(exist_ok=True)

            vocab = loaded_tokenizer.get_vocab()
            id_to_token = {v: k for k, v in vocab.items()}
            fast_tokenizer = cls.SUB_TOKENIZER_CLASS(
                vocab_file=empty_vocab_file_path,
                **kwargs,
            )
            fast_tokenizer.add_tokens([id_to_token[i] for i in range(len(id_to_token))])

            updated_vocab = fast_tokenizer.get_vocab()
            sorted_vocab = [
                key
                for key, idx in sorted(updated_vocab.items(), key=lambda item: item[1])
            ]
            vocab_file_path = f"{tmpdir}/created_vocab.txt"
            with open(vocab_file_path, "w") as vocab_file:
                vocab_file.write("\n".join(sorted_vocab) + "\n")

            updated_tokenizer = cls.SUB_TOKENIZER_CLASS(vocab_file=vocab_file_path)
            save_directory = Path(tmpdir) / "updated_tokenizer"
            updated_tokenizer.save_pretrained(
                save_directory=save_directory,
            )
            full_tokenizer = cls.SUB_TOKENIZER_CLASS.from_pretrained(
                pretrained_model_name_or_path=save_directory, **kwargs
            )
            if save_converted_tokenizer_back:
                full_tokenizer.save_pretrained(save_directory=name_or_path)
                full_tokenizer = cls.SUB_TOKENIZER_CLASS.from_pretrained(name_or_path)
        return full_tokenizer

    @classmethod
    def from_sub_tokenizers(cls, field_to_tokenizer_map: dict = None, **kwargs):
        """
        note: this method should only be used for new configurations.  Otherwise, use
            multi_field_tokenizer = MultiFieldTokenizer.create_new_multi_field_tokenizer_from_sub_tokenizers(field_dictionary)
            multi_field_tokenizer.save_pretrained(path_for_new_tokenizer).

        just once, and from now on you can use
            multi_field_tokenizer = AutoTokenizer.from_pretraind(path_for_new_tokenizer)

        We meed a dictionary of field_name:field_tokenizer_path

        Args:
        ----
            mf_paths_dict (dict[field_name:str -> path_to_saved_tokenizer:str]): field to tokenizer dictionary.

        Returns:
        -------
            MultiFieldTokenizer: created tokenizer

        """
        if not isinstance(field_to_tokenizer_map, dict):
            raise ValueError(
                "multifield tokenizer must be created from a dictionary of tokenizers, got %s"
                % str(field_to_tokenizer_map),
            )
        if field_to_tokenizer_map:
            field_to_tokenizer_map = {
                key: str(value) for key, value in field_to_tokenizer_map.items()
            }
        new_multi_field_tokenizer = MultiFieldTokenizer(
            field_to_tokenizer_map=field_to_tokenizer_map,
            **kwargs,
            load_relative=False,
        )
        new_multi_field_tokenizer.load_all_subtokenizers()

        # to make sure that the tokenizer will save and then load properly, we save and load it and use the loaded version
        # this should take care of any potentioal issues due to this bootstrup process of creating a tokenizer
        tmpdir = tempfile.TemporaryDirectory()
        # the temprorary directory has to live untill the program exits, as we may need to re-read the file (for vocab reset)
        atexit.register(tmpdir.cleanup)
        new_multi_field_tokenizer.save_pretrained(tmpdir.name)

        multi_field_tokenizer: MultiFieldTokenizer = (
            MultiFieldTokenizer.from_pretrained(
                tmpdir.name,
                field_to_tokenizer_map=field_to_tokenizer_map,
                **kwargs,
                load_relative=True,
            )
        )
        return multi_field_tokenizer

    def load_subtokenizer(self, field_name: str, field_to_tokenizer_map=None):
        """
        load a sub tokenizer from disk.

        field_name:
            name (str): name of tokenizer to load
        """
        if field_to_tokenizer_map is None:
            field_to_tokenizer_map = self.field_to_tokenizer_map
            path = field_to_tokenizer_map.get(field_name)
            if self.base_dir:
                path = f"{self.base_dir}/{field_name}"
        else:
            path = field_to_tokenizer_map.get(field_name)
        if path is None:
            raise ValueError(
                "Failed to load tokenizer for field %s as no path is present in the subtokenizer dictionary",
                field_name,
            )
        try:
            loaded_tok = self.SUB_TOKENIZER_CLASS.from_pretrained(
                path,
                do_lower_case=False,
                tokenize_chinese_chars=False,
                clean_text=False,
                strip_accents=None,
            )
        except Exception as e:
            logger.error(
                f"failed to load {path} as a {self.SUB_TOKENIZER_CLASS.__name__}"
            )
            raise e

        self.tokenizers[field_name] = loaded_tok

    def load_all_subtokenizers(self):
        if self.base_dir:
            path = Path(self.base_dir)
            self.field_to_tokenizer_map = {}
            for file in path.iterdir():
                if file.is_dir():
                    name = file.name
                    self.field_to_tokenizer_map[name] = name
        for key in self.field_to_tokenizer_map:
            self.load_subtokenizer(key)
        self.sanitize_special_tokens()

    #### tokenizer functionality duplicated - only things that actually get used

    @property
    def _default_tokenizer(self):
        if self.default_field is not None:
            default_field_name = self.default_field
        else:
            default_field_name = sorted(self.tokenizers.keys())[0]
        return self.get_field_tokenizer(default_field_name)

    @property
    def mask_token(self):
        return self._default_tokenizer.mask_token

    @property
    def mask_token_id(self):
        return self._default_tokenizer.mask_token_id

    @property
    def cls_token_id(self):
        return self._default_tokenizer.cls_token_id

    @property
    def sep_token_id(self):
        return self._default_tokenizer.sep_token_id

    @property
    def unk_token_id(self):
        return self._default_tokenizer.unk_token_id

    @property
    def unk_token(self):
        return self._default_tokenizer.unk_token

    @property
    def pad_token_id(self):
        return self._default_tokenizer.pad_token_id

    @property
    def padding_side(self):
        return self._default_tokenizer.padding_side

    @property
    def pad_token(self):
        return self._default_tokenizer.pad_token

    @property
    def all_special_ids(self):
        return self._default_tokenizer.all_special_ids

    @property
    def all_special_tokens(self):
        return self._default_tokenizer.all_special_tokens

    @property
    def is_fast(self):
        return True

    def __call__(
        self,
        mfi: MultiFieldInstance | list[MultiFieldInstance],
        mfi_pair: MultiFieldInstance | list[MultiFieldInstance] | None = None,
        fields: list[FieldInfo] | None = None,
        add_special_tokens: bool = True,
        padding: bool | str | PaddingStrategy = False,
        truncation: bool | str | TruncationStrategy = True,
        max_length: int | None = None,
        stride: int = 0,
        is_split_into_words: bool = True,  # for multifield type input
        pad_to_multiple_of: int | None = None,
        return_tensors: str | TensorType | None = None,
        return_token_type_ids: bool | None = None,
        return_attention_mask: bool | None = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = True,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        **kwargs,
    ) -> BatchEncoding:
        """
        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
        sequences.

        Args:
        ----
            mfi (`MultiFieldInstance`, `List[MultiFieldInstance]`, `List[List[MultiFieldInstance]]`, *optional*):
                The sequence or batch of sequences to be encoded.
            mfi_pair (`MultiFieldInstance`, `List[MultiFieldInstance]`, `List[List[MultiFieldInstance]]`, *optional*):
                The sequence or batch of sequences to be encoded.
        """
        # To avoid duplicating
        all_kwargs = {
            "add_special_tokens": add_special_tokens,
            "padding": padding,
            "truncation": truncation,
            "max_length": max_length,
            "stride": stride,
            "is_split_into_words": is_split_into_words,
            "pad_to_multiple_of": pad_to_multiple_of,
            "return_tensors": return_tensors,
            "return_token_type_ids": return_token_type_ids,
            "return_attention_mask": return_attention_mask,
            "return_overflowing_tokens": return_overflowing_tokens,
            "return_special_tokens_mask": return_special_tokens_mask,
            "return_offsets_mapping": return_offsets_mapping,
            "return_length": return_length,
            "verbose": verbose,
        }
        all_kwargs.update(kwargs)
        if isinstance(mfi, MultiFieldInstance):
            mfi = [mfi]
        encodings = {}

        present_field_names = mfi[0].keys()
        active_fields = [f for f in fields if f.field_name in present_field_names]
        inactive_fields = [f for f in fields if f.field_name not in present_field_names]
        # only non input fields are permitted to be missing
        # if a field is expected as an input, it must be present
        assert not any(f.is_input for f in inactive_fields)

        for field in active_fields:
            if field.tokenization_strategy == "tokenize":
                field_tokenizer = self.get_field_tokenizer(field.field_name)
            else:
                field_tokenizer = self._default_tokenizer
            field_inputs = [
                [_stringify(x[field.field_name], is_split_into_words) for x in mfi]
            ]
            if mfi_pair is not None:
                field_inputs.append(
                    [
                        _stringify(x[field.field_name], is_split_into_words)
                        for x in mfi_pair
                    ]
                )
            field_encodings = field_tokenizer(*field_inputs, **all_kwargs)

            if field.tokenization_strategy == "continuous_value_encoder":
                field_encodings[
                    "input_ids"
                ] = self._replace_tokenized_ids_with_continuous_values(
                    mfi,
                    mfi_pair,
                    field=field,
                    input_ids=field_encodings["input_ids"].float(),
                    added_specials=field_encodings["special_tokens_mask"],
                    token_type_ids=field_encodings["token_type_ids"],
                )

            encodings[field.field_name] = field_encodings
        return BatchEncoding(data=encodings)

    def _replace_tokenized_ids_with_continuous_values(
        self,
        mfi: list[MultiFieldInstance],
        mfi_pair: list[MultiFieldInstance] | None,
        field: FieldInfo,
        input_ids: torch.Tensor,
        added_specials: torch.Tensor,
        token_type_ids: torch.Tensor,
    ):
        for this_mfi, token_type_id in [(mfi, 0), (mfi_pair, 1)]:
            if this_mfi is None:
                continue
            update_idx = ((~added_specials) & (token_type_ids == token_type_id)).bool()
            for sample_idx, x in enumerate(this_mfi):
                sample_update_idx = update_idx[sample_idx]
                trunc_sample_len = sample_update_idx.sum()
                arr = torch.tensor(
                    x[field.field_name][:trunc_sample_len], dtype=input_ids.dtype
                )
                input_ids[sample_idx][sample_update_idx] = arr

        return input_ids

    def add_tokens_for_field(self, field_name, token_to_add):
        self.get_field_tokenizer(field_name).add_tokens(token_to_add)

    def get_field_vocab(self, field_name: str) -> dict[str, int]:
        """
        Gets the tokens for a given field.

        Args:
        ----
            field (str): The key for the field.

        Returns:
        -------
            dict[str, int]: The tokens for the field, with their ids

        """
        tokenizer: Tokenizer = self.tokenizers[field_name]
        return tokenizer.get_vocab()

    def field_vocab_size(self, name):
        return self.get_field_tokenizer(name).vocab_size

    def get_field_tokenizer(
        self, field: str
    ) -> PreTrainedTokenizer | PreTrainedTokenizerFast:
        """
        get a specific sub tokenizer.

        Args:
        ----
            field (str): name of field

        Returns:
        -------
            Tokenizer: tokenizer for this field

        """
        return self.tokenizers[field]

    def reset_tokenizer_vocab(self, field_name: str):
        """
        reload a tokenizer from disk.

        Args:
        ----
            field_name (str): name of field for which the subtokenizer will be reset.

        """
        self.load_subtokenizer(field_name)

    def get_special_token_binary_mask(self, field: str) -> list[bool]:
        """
        Returns the special token mask for a given field.
        Used (only) in test_metrics.  TODO: is this needed?.

        Args:
        ----
            field (str): The field to get the special token mask for.

        Returns:
        -------
            List[bool]: The special token mask for the given field.

        """
        if field not in self.tokenizers.keys():
            raise ValueError(
                f"field {field} not found in the vocabulary. Available fields: {self.tokenizers.keys()}"
            )
        special_tokens = self.all_special_ids
        return [
            token_id in special_tokens
            for token_id in sorted(self.get_field_vocab(field).values())
        ]

    def convert_field_ids_to_tokens(
        self,
        ids: int | list[int],
        skip_special_tokens: bool = False,
        field: str = "expressions",
    ) -> str | list[str]:
        """
        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
        added tokens.

        Args:
        ----
            ids (`int` or `List[int]`):
                The token id (or token ids) to convert to tokens.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.

        Returns:
        -------
            `str` or `List[str]`: The decoded token(s).

        """
        return self.get_field_tokenizer(field).convert_ids_to_tokens(
            ids, skip_special_tokens
        )

    def convert_field_tokens_to_ids(self, field: str, tokens: list[str]) -> list[int]:
        """
        Converts a list of tokens to their corresponding IDs.

        Args:
        ----
            tokens (Union[str, List[str]]): The tokens to convert.
            field (str): The field the tokens belong to.

        Returns:
        -------
            int or List[int]: The IDs of the tokens.

        """
        return self.get_field_tokenizer(field).convert_tokens_to_ids(tokens=tokens)

    def get_token_values(self, field: str) -> list[float] | None:
        """
        Get float values for field, if available.

        Note: list must be all floats so it can be copied to cuda device

        Args:
        ----
            field (str): tokenizer field

        Returns:
        -------
            list[float] | None: list of float values with NaN for special tokens and
                numbers for other tokens or None if ANY of the non-special tokens are
                not castable to numbers

        """
        field_tokens = self.get_field_vocab(field)

        def num(t):
            try:
                return float(int(t))
            except:
                return float("nan")

        reversed_tokens = {v: k for k, v in field_tokens.items()}
        token_values = [num(reversed_tokens[t]) for t in sorted(reversed_tokens.keys())]
        if sum(map(math.isnan, token_values)) > len(self.all_special_tokens):
            logger.warn("Field %s cannot be converted to float values.", field)
            return None
        return token_values

    def sanitize_special_tokens(self, all_special_tokens_must_match=False):
        """
        Sanitizes the special tokens.  Mostly just verifies that all the special token ids match.

        Raises
        ------
            ValueError: If the special tokens are not the same for all fields.

        """
        if all_special_tokens_must_match:
            special_token_ids = [
                self.get_field_tokenizer(field_name).all_special_ids
                for field_name in self.tokenizers.keys()
            ]
            special_token_ids.append(self.all_special_ids)
            from itertools import product

            for i, j in product(special_token_ids, repeat=2):
                assert i == j
